
*opencorporates* : Base de datos global de compañías
*bgp toolkit* : Podemos ver los rango de IP de la compañía y nombres DNS
*shoadan* : Motor de búsqueda

Una vez tengamos una lista con los dominios encontrados deberemos seleccionar el dominio al que vamos a apuntar.

Empezaremos a buscar sobre ese dominio con Google Dorking. también podemos buscar por GitHub, paneles de administración con Google.

Podemos usar la web de *host.io* para buscar información sobre dominios
Podemos encontrar *backnames* que son empresas que apuntan a la compañía principal
La pagina de *built with* en la parte de Relationships nos muestra nos muestra los id de Google analytics y podemos ver el funcionamiento de la empresa y poder sacar información de dominios y subdominios

Para buscar APIs de una empresa podemos usar *postman.com* y *swagger.io* 

También podemos usar herramientas de GitHub como por ejemplo:
*dorks_hunter* Automatizar búsquedas en Google, busca solo información interesante.
*SwaggerSpy* Automatiza las búsquedas en la web de Swagger.io
*porch-pirate* Automatiza la búsquedas en la web de Postman.com
*metagoofil* Automatiza la búsqueda de documentos ofimáticos mediante Google Dorks pudiendo filtrar podemos usar para hacer el filtrado la siguiente herramienta junto con la regex a filtar  `exiftool -r /ruta/archivos/descargados | egrep -i "Author|Creator|Email|Producer|Template"`

*LeakSearch* Automatiza la búsqueda de emails y contraseñas si se han filtrado de la empresa


# 01. RECOLECCION DE SUBDOMINIOS

Para la enumeración de subdominios se puede hacer de forma pasiva usando servicios de terceros o de forma activa que es interactuando con nuestro objetivo

Se puede utilizar para la enumeración pasiva *subfinder* y *amass* también se puede hacer por búsqueda de certificados con *crt*
Con *crt* si lo queremos guardar para poder automatizarlo en un futuro usaremos
`crt -s -json empresa.co | jq -r ".[].subdomain" | anew -q /ruta/donde/guardar/info.txt`

Para hacer resoluciones DNS masivos podemos usar herramientas como *puredns*, los resolvers los sacamos del GitHub de trickest-resolvers
`puredns resolve /ruta/guadado/subdominios.txt -r /ruta/reoslvers/DNS.txt --resolvers-trusted /ruta/resolvers-trusted.txt -w /ruta/donde/guardar/resultado.txt`

Para hacer la resolución DNS por fuerza bruta para encontrar nuevos dominios usaremos de nuevo *puredns*
`puredns bruteforce /diccionari/furza/bruta dominio/subdominio.com -r /ruta/reoslvers/DNS.txt --resolvers-trusted /ruta/resolvers-trusted.txt -w /ruta/donde/guardar/resultado.txt`

Con los subdominios encontrados podemos hacer un reconocimiento de los tipos de registro usamos la herramienta *dnsx*
`cat /ruta/subdominios/validos.txt | dnsx -recon -resp | cut -d " " -f3 | grep "empresa\.com"`

Con los certificados también podemos sacar mas subdominios con la herramienta *tlsx*
`cat /ruta/subdominios/validos.txt | tlsx -san -cn -ro -c -o /ruta/guardar/resultado.txt`


# 02. HOST & CLOUD

Pasar de subdominios -> DNS tipo A -> IP
Para guardar los registros DNS de tipo A usaremos la herramienta *dnsx*
`cat /ruta/subdominios/validos.txt | dnsx -recon -json -resp -o /ruta/guarda/result.txt`

La salida que nos da en json, lo que nos interesa son los host
`cat /ruta/guarda/result.txt | jq -r "try .a" > /ruta/guardar/Ips.txt`

ahora separaremos las IPs de los rangos de la nube, waf y cdn de las demás usaremos la herramienta *cdncheck*
`cat /ruta/ips.txt | cdncheck -resp -cdn -waf -nc`

En el resultado todas las IPs que no pertenezcan a los rangos de puertos de cdn y waf les haremos los escaneos de puertos

Para este escaneo usaremos *smap*
`smap -iL /ruta/ips.txt -oA /ruta/guradar/salida`

Usaremos la herramienta *nmapurls* para obtener los dominios de esas IPs
`nmapurl -f /ruta/salida/anterior`



# 03. WEB & URL

Usaremos una herramienta llamada *httpx* que nos mostrara las tecnologías que funciona en la web
`cat /ruta/subdominios/validos.txt | httpx -follow-redirects -random-agent -status-code -title -web-server -tech-detect -location -content-length -o /ruta/donde/guardar.txt`

En esta parte podemos empezar ya a buscar nuestros objetivos

En este punto podemos empezar a identificar que sitios web se esta ejecutando un WAF usaremos la herramienta *wafw00f* nos centraremos solo en los que no tienen WAF
`wafw00f -i /ruta/anterior/webs.txt`

Para sacar un pantallazo visual de cada sitio que hemos sacado usaremos *gowitness*
`gowitness scan file -f /ruta/webs.txt`

Para buscar CMS usaremos la herramienta *cmsseek* no se le puede mandar una lista con sitios web para eso habría que hacer un bucle y que les vaya pasando uno por uno las URL
`cmsseek.py -u enlace.com`

Para recolectar todas URLs que pertenecen a un dominio de forma pasiva usaremos la herramienta *urlfinder* 
`urlfinder -d empresa.com -o /ruta/archivo/guardar/url.txt`

Para recolectar todas URLs que pertenecen a un dominio de forma activa usaremos la herramienta *katana*
`katana -list -/ruta/webs.txt -jc -kf all -d e -fs rdn -o /ruta/archivo/url_activas.txt`

Una vez con toda esta información debemos identificar que están activas con *httpx*
`cat /rutas/guardar/url.txt | httpx`
Haremos lo mismo con las de forma activa

Ahora tendremos que hacer FUZZING usaremos la herramienta *fuff*

